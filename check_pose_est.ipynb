{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "checking the pose estimation network and the loss to get the more better understanding of the architecture\n",
    "and the losses and still have to write the 3d geometrical loss in temporal loss\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Conv2DTranspose,UpSampling2D,MaxPooling2D,Concatenate,BatchNormalization,Flatten\n",
    "from tensorflow.keras.layers import Dense,Layer\n",
    "import numpy as np\n",
    "from losses import *\n",
    "from nets.dep_network import depth_estimation\n",
    "from nets.pose_network import pose_est\n",
    "from data_loader import data_loader_with_batch\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler2mat(z, y, x):\n",
    "    \"\"\"Converts euler angles to rotation matrix\n",
    "    TODO: remove the dimension for 'N' (deprecated for converting all source\n",
    "         poses altogether)\n",
    "    Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n",
    "    Args:\n",
    "      z: rotation angle along z axis (in radians) -- size = [B, N]\n",
    "      y: rotation angle along y axis (in radians) -- size = [B, N]\n",
    "      x: rotation angle along x axis (in radians) -- size = [B, N]\n",
    "    Returns:\n",
    "      Rotation matrix corresponding to the euler angles -- size = [B, N, 3, 3]\n",
    "    \"\"\"\n",
    "    B = tf.shape(z)[0]\n",
    "    N = 1\n",
    "    z = tf.clip_by_value(z, -np.pi, np.pi)\n",
    "    y = tf.clip_by_value(y, -np.pi, np.pi)\n",
    "    x = tf.clip_by_value(x, -np.pi, np.pi)\n",
    "\n",
    "    # Expand to B x N x 1 x 1\n",
    "    z = tf.expand_dims(tf.expand_dims(z, -1), -1)\n",
    "    y = tf.expand_dims(tf.expand_dims(y, -1), -1)\n",
    "    x = tf.expand_dims(tf.expand_dims(x, -1), -1)\n",
    "\n",
    "    zeros = tf.zeros([B, N, 1, 1])\n",
    "    ones  = tf.ones([B, N, 1, 1])\n",
    "\n",
    "    cosz = tf.cos(z)\n",
    "    sinz = tf.sin(z)\n",
    "    rotz_1 = tf.concat([cosz, -sinz, zeros], axis=3)\n",
    "    rotz_2 = tf.concat([sinz,  cosz, zeros], axis=3)\n",
    "    rotz_3 = tf.concat([zeros, zeros, ones], axis=3)\n",
    "    zmat = tf.concat([rotz_1, rotz_2, rotz_3], axis=2)\n",
    "\n",
    "    cosy = tf.cos(y)\n",
    "    siny = tf.sin(y)\n",
    "    roty_1 = tf.concat([cosy, zeros, siny], axis=3)\n",
    "    roty_2 = tf.concat([zeros, ones, zeros], axis=3)\n",
    "    roty_3 = tf.concat([-siny,zeros, cosy], axis=3)\n",
    "    ymat = tf.concat([roty_1, roty_2, roty_3], axis=2)\n",
    "\n",
    "    cosx = tf.cos(x)\n",
    "    sinx = tf.sin(x)\n",
    "    rotx_1 = tf.concat([ones, zeros, zeros], axis=3)\n",
    "    rotx_2 = tf.concat([zeros, cosx, -sinx], axis=3)\n",
    "    rotx_3 = tf.concat([zeros, sinx, cosx], axis=3)\n",
    "    xmat = tf.concat([rotx_1, rotx_2, rotx_3], axis=2)\n",
    "\n",
    "    rotMat = tf.matmul(tf.matmul(xmat, ymat), zmat)\n",
    "    return rotMat\n",
    "\n",
    "def pose_vec2mat(translation,rotation):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "      vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "    Returns:\n",
    "      A transformation matrix -- [B, 4, 4]\n",
    "    \"\"\"\n",
    "    batch_size, _ = translation.get_shape().as_list()\n",
    "    translation = tf.expand_dims(translation, -1)\n",
    "    rx = tf.slice(rotation, [0, 0], [-1, 1])\n",
    "    ry = tf.slice(rotation, [0, 1], [-1, 1])\n",
    "    rz = tf.slice(rotation, [0, 2], [-1, 1])\n",
    "    rot_mat = euler2mat(rz, ry, rx)\n",
    "    rot_mat = tf.squeeze(rot_mat, axis=[1])\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch_size, 1, 1])\n",
    "    transform_mat = tf.concat([rot_mat, translation], axis=2)\n",
    "    transform_mat = tf.concat([transform_mat, filler], axis=1)\n",
    "    return transform_mat\n",
    "\n",
    "def pose_inv_vec2mat(translation,rotation):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "      vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "    Returns:\n",
    "      A transformation matrix -- [B, 4, 4]\n",
    "    \"\"\"\n",
    "    batch_size, _ = translation.get_shape().as_list()\n",
    "    translation = tf.expand_dims(translation, -1)\n",
    "    rx = tf.slice(rotation, [0, 0], [-1, 1])\n",
    "    ry = tf.slice(rotation, [0, 1], [-1, 1])\n",
    "    rz = tf.slice(rotation, [0, 2], [-1, 1])\n",
    "    rot_mat = euler2mat(rz, ry, rx)\n",
    "    rot_mat_transpose = tf.transpose(tf.squeeze(rot_mat, axis=[1]))\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch_size, 1, 1])\n",
    "    inv_homo_trans = -tf.matmul(rot_mat_transpose,translation)\n",
    "    transform_mat = tf.concat([rot_mat, inv_homo_trans], axis=2)\n",
    "    transform_mat = tf.concat([transform_mat, filler], axis=1)\n",
    "    return transform_mat\n",
    "\n",
    "def pixel2cam(depth, pixel_coords, intrinsics, is_homogeneous=True):\n",
    "    \"\"\"Transforms coordinates in the pixel frame to the camera frame.\n",
    "    Args:\n",
    "    depth: [batch, height, width]\n",
    "    pixel_coords: homogeneous pixel coordinates [batch, 3, height, width]\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    is_homogeneous: return in homogeneous coordinates\n",
    "    Returns:\n",
    "    Coords in the camera frame [batch, 3 (4 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    batch, height, width = depth.get_shape().as_list()\n",
    "    depth = tf.reshape(depth, [batch, 1, -1])\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 3, -1])\n",
    "    cam_coords = tf.matmul(tf.linalg.inv(intrinsics), pixel_coords) * depth\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones([batch, 1, height*width])\n",
    "        cam_coords = tf.concat([cam_coords, ones], axis=1)\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, -1, height, width])\n",
    "    return cam_coords\n",
    "\n",
    "def cam2pixel(cam_coords, proj):\n",
    "    \"\"\"Transforms coordinates in a camera frame to the pixel frame.\n",
    "    Args:\n",
    "    cam_coords: [batch, 4, height, width]\n",
    "    proj: [batch, 4, 4]\n",
    "    Returns:\n",
    "    Pixel coordinates projected from the camera frame [batch, height, width, 2]\n",
    "    \"\"\"\n",
    "    batch, _, height, width = cam_coords.get_shape().as_list()\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, 4, -1])\n",
    "    unnormalized_pixel_coords = tf.matmul(proj, cam_coords)\n",
    "    x_u = tf.slice(unnormalized_pixel_coords, [0, 0, 0], [-1, 1, -1])\n",
    "    y_u = tf.slice(unnormalized_pixel_coords, [0, 1, 0], [-1, 1, -1])\n",
    "    z_u = tf.slice(unnormalized_pixel_coords, [0, 2, 0], [-1, 1, -1])\n",
    "    x_n = x_u / (z_u + 1e-10)\n",
    "    y_n = y_u / (z_u + 1e-10)\n",
    "    pixel_coords = tf.concat([x_n, y_n], axis=1)\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 2, height, width])\n",
    "    return tf.transpose(pixel_coords, perm=[0, 2, 3, 1])\n",
    "\n",
    "def meshgrid(batch, height, width, is_homogeneous=True):        \n",
    "    \"\"\"Construct a 2D meshgrid.\n",
    "    Args:\n",
    "    batch: batch size\n",
    "    height: height of the grid\n",
    "    width: width of the grid\n",
    "    is_homogeneous: whether to return in homogeneous coordinates\n",
    "    Returns:\n",
    "    x,y grid coordinates [batch, 2 (3 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n",
    "                  tf.transpose(tf.expand_dims(\n",
    "                      tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n",
    "    y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n",
    "                  tf.ones(shape=tf.stack([1, width])))\n",
    "    x_t = (x_t + 1.0) * 0.5 * tf.cast(width - 1, tf.float32)\n",
    "    y_t = (y_t + 1.0) * 0.5 * tf.cast(height - 1, tf.float32)\n",
    "    \"\"\"\n",
    "    width_f = tf.cast(width,tf.float32)\n",
    "    height_f = tf.cast(height,tf.float32)\n",
    "    x_t,y_t = tf.meshgrid(tf.linspace(0.0,width_f,width),tf.linspace(0.0,height_f,height))\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones_like(x_t)\n",
    "        coords = tf.stack([x_t, y_t, ones], axis=0)\n",
    "    else:\n",
    "        coords = tf.stack([x_t, y_t], axis=0)\n",
    "    coords = tf.tile(tf.expand_dims(coords, 0), [batch, 1, 1, 1])\n",
    "    return coords\n",
    "\n",
    "def projective_inverse_warp(img, depth, translate,rotate, intrinsics,inv=False):\n",
    "    \"\"\"Inverse warp a source image to the target image plane based on projection.\n",
    "    Args:\n",
    "    img: the source image [batch, height_s, width_s, 3]\n",
    "    depth: depth map of the target image [batch, height_t, width_t]\n",
    "    pose: target to source camera transformation matrix [batch, 6], in the\n",
    "          order of tx, ty, tz, rx, ry, rz\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    Returns:\n",
    "    Source image inverse warped to the target image plane [batch, height_t,\n",
    "    width_t, 3]\n",
    "    \"\"\"\n",
    "    batch, height, width, _ = img.get_shape().as_list()\n",
    "    # Convert pose vector to matrix\n",
    "    pose = pose_vec2mat(translate,rotate)\n",
    "    if inv==False:\n",
    "        pose=pose\n",
    "    else:\n",
    "        pose = tf.linalg.inv(pose)\n",
    "    # Construct pixel grid coordinates\n",
    "    pixel_coords = meshgrid(batch, height, width)\n",
    "    # Convert pixel coordinates to the camera frame\n",
    "    cam_coords = pixel2cam(depth, pixel_coords, intrinsics)\n",
    "    # Construct a 4x4 intrinsic matrix (TODO: can it be 3x4?)\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch, 1, 1])\n",
    "    intrinsics = tf.concat([intrinsics, tf.zeros([batch, 3, 1])], axis=2)\n",
    "    intrinsics = tf.concat([intrinsics, filler], axis=1)\n",
    "    # Get a 4x4 transformation matrix from 'target' camera frame to 'source'\n",
    "    # pixel frame.\n",
    "    proj_tgt_cam_to_src_pixel = tf.matmul(intrinsics, pose)\n",
    "    src_pixel_coords = cam2pixel(cam_coords, proj_tgt_cam_to_src_pixel)\n",
    "    output_img = bilinear_sampler(img, src_pixel_coords)\n",
    "    return output_img \n",
    "def bilinear_sampler(imgs, coords):\n",
    "    \"\"\"Construct a new image by bilinear sampling from the input image.\n",
    "    Points falling outside the source image boundary have value 0.\n",
    "    Args:\n",
    "    imgs: source image to be sampled from [batch, height_s, width_s, channels]\n",
    "    coords: coordinates of source pixels to sample from [batch, height_t,\n",
    "      width_t, 2]. height_t/width_t correspond to the dimensions of the output\n",
    "      image (don't need to be the same as height_s/width_s). The two channels\n",
    "      correspond to x and y coordinates respectively.\n",
    "    Returns:\n",
    "    A new sampled image [batch, height_t, width_t, channels]\n",
    "    \"\"\"\n",
    "    def _repeat(x, n_repeats):        \n",
    "        rep = tf.transpose(\n",
    "            tf.expand_dims(tf.ones(shape=tf.stack([\n",
    "                n_repeats,\n",
    "            ])), 1), [1, 0])\n",
    "        rep = tf.cast(rep, 'float32')\n",
    "        x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "        return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "    coords_x, coords_y = tf.split(coords, [1, 1], axis=3)\n",
    "    inp_size = imgs.get_shape()\n",
    "    coord_size = coords.get_shape()\n",
    "    out_size = coords.get_shape().as_list()\n",
    "    out_size[3] = imgs.get_shape().as_list()[3]\n",
    "\n",
    "    coords_x = tf.cast(coords_x, 'float32')\n",
    "    coords_y = tf.cast(coords_y, 'float32')\n",
    "\n",
    "    x0 = tf.floor(coords_x)\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.floor(coords_y)\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    y_max = tf.cast(tf.shape(imgs)[1] - 1, 'float32')\n",
    "    x_max = tf.cast(tf.shape(imgs)[2] - 1, 'float32')\n",
    "    zero = tf.zeros([1], dtype='float32')\n",
    "\n",
    "    x0_safe = tf.clip_by_value(x0, zero, x_max)\n",
    "    y0_safe = tf.clip_by_value(y0, zero, y_max)\n",
    "    x1_safe = tf.clip_by_value(x1, zero, x_max)\n",
    "    y1_safe = tf.clip_by_value(y1, zero, y_max)\n",
    "\n",
    "    ## bilinear interp weights, with points outside the grid having weight 0\n",
    "    # wt_x0 = (x1 - coords_x) * tf.cast(tf.equal(x0, x0_safe), 'float32')\n",
    "    # wt_x1 = (coords_x - x0) * tf.cast(tf.equal(x1, x1_safe), 'float32')\n",
    "    # wt_y0 = (y1 - coords_y) * tf.cast(tf.equal(y0, y0_safe), 'float32')\n",
    "    # wt_y1 = (coords_y - y0) * tf.cast(tf.equal(y1, y1_safe), 'float32')\n",
    "\n",
    "    wt_x0 = x1_safe - coords_x\n",
    "    wt_x1 = coords_x - x0_safe\n",
    "    wt_y0 = y1_safe - coords_y\n",
    "    wt_y1 = coords_y - y0_safe\n",
    "\n",
    "    ## indices in the flat image to sample from\n",
    "    dim2 = tf.cast(inp_size[2], 'float32')\n",
    "    dim1 = tf.cast(inp_size[2] * inp_size[1], 'float32')\n",
    "    base = tf.reshape(\n",
    "        _repeat(\n",
    "            tf.cast(tf.range(coord_size[0]), 'float32') * dim1,\n",
    "            coord_size[1] * coord_size[2]),\n",
    "        [out_size[0], out_size[1], out_size[2], 1])\n",
    "\n",
    "    base_y0 = base + y0_safe * dim2\n",
    "    base_y1 = base + y1_safe * dim2\n",
    "    idx00 = tf.reshape(x0_safe + base_y0, [-1])\n",
    "    idx01 = x0_safe + base_y1\n",
    "    idx10 = x1_safe + base_y0\n",
    "    idx11 = x1_safe + base_y1\n",
    "\n",
    "    ## sample from imgs\n",
    "    imgs_flat = tf.reshape(imgs, tf.stack([-1, inp_size[3]]))\n",
    "    imgs_flat = tf.cast(imgs_flat, 'float32')\n",
    "    im00 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx00, 'int32')), out_size)\n",
    "    im01 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx01, 'int32')), out_size)\n",
    "    im10 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx10, 'int32')), out_size)\n",
    "    im11 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx11, 'int32')), out_size)\n",
    "\n",
    "    w00 = wt_x0 * wt_y0\n",
    "    w01 = wt_x0 * wt_y1\n",
    "    w10 = wt_x1 * wt_y0\n",
    "    w11 = wt_x1 * wt_y1\n",
    "\n",
    "    output = tf.add_n([\n",
    "        w00 * im00, w01 * im01,\n",
    "        w10 * im10, w11 * im11\n",
    "    ])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe = pose_est()\n",
    "def pose_est_loss(rot_right,rot_left,t_left,t_right):\n",
    "    ##Spatial loss\n",
    "    #Pose consistency loss\n",
    "    lambd_p = 0.5\n",
    "    lambd_o = 0.5\n",
    "\n",
    "    pc_loss = lambd_p*tf.reduce_mean(tf.abs(t_right-t_left))+lambd_o*tf.reduce_mean(tf.abs(rot_right-rot_left))\n",
    "\n",
    "    #Temporal loss\n",
    "    \n",
    "    return pc_loss\n",
    "\n",
    "def pose_temporal_loss(pk,pk1,translate,rotate,intrinsics,depth):\n",
    "    \"\"\"\n",
    "    Argument :\n",
    "    pk is the pixel in the current frame \n",
    "    pk1 which is p_k+1 is the next frame pixel or next image \n",
    "    \"\"\"\n",
    "    ##loss for predicting the pk+1\n",
    "    lambd = 0.5\n",
    "    pk1_pred = projective_inverse_warp(pk, depth, translate,rotate, intrinsics)\n",
    "    ssim_loss_pk1 = SSIM(pk1,pk1_pred) \n",
    "    l1_loss_pk1 = tf.reduce_mean(tf.abs(tf.subtract(pk1_pred,pk1)))\n",
    "    loss_pk1 = lambd*ssim_loss_pk1+(1-lambd)*l1_loss_pk1\n",
    "    \n",
    "    ##loss for predicting the pk\n",
    "    lambd = 0.5\n",
    "    pk_pred = projective_inverse_warp(pk, depth, translate,rotate, intrinsics,inv=True)\n",
    "    ssim_loss_pk = SSIM(pk,pk_pred) \n",
    "    l1_loss_pk = tf.reduce_mean(tf.abs(tf.subtract(pk_pred,pk)))\n",
    "    loss_pk = lambd*ssim_loss_pk+(1-lambd)*l1_loss_pk\n",
    "    \n",
    "    photo_temporal_loss = loss_pk+loss_pk1\n",
    "    \n",
    "    return photo_temporal_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "batch=2        \n",
    "epsilon=0.0000001\n",
    "@tf.function\n",
    "def train_step(image_left,image_right):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        intrinsic = tf.expand_dims(tf.constant([[1,2,3],[4,2,4],[2,1,6]],dtype=tf.float32),axis=0)\n",
    "        intrinsic = tf.tile(intrinsic,[batch,1,1])\n",
    "\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        dcx7_right,dep_right = de(image_right)\n",
    "        #dcx7_left,dep_left = de(image_left)\n",
    "        #loss_depth = dept_est_loss(dep_left,dep_right,image_left,image_right)\n",
    "        #tf.print(\"loss_depth\",loss_depth)\n",
    "        ##not using pose estimation currently because of tensorflow 2.0\n",
    "        t_left,rot_left = pe(image_left)\n",
    "        t_right,rot_right = pe(image_right)\n",
    "        loss_pose = pose_est_loss(rot_right,rot_left,t_left,t_right)\n",
    "        disp = (dep_right[:,:,:,0]+epsilon)\n",
    "        depth = 1/disp\n",
    "        pk = tf.slice(image_right,[0,0,0,0],[-1,-1,-1,3])\n",
    "        pk1 = tf.slice(image_right,[0,0,0,3],[-1,-1,-1,3])\n",
    "        loss_pose_1 = pose_temporal_loss(pk,pk1,t_right,rot_right,intrinsic,depth)\n",
    "        #tf.print(\"loss_pose\",loss_pose)\n",
    "        loss_pose_total = loss_pose+loss_pose_1\n",
    "\n",
    "    #gradients_de = tape.gradient(loss_depth, de.trainable_variables)\n",
    "    gradients_pe = tape.gradient(loss_pose_total, pe.trainable_variables)\n",
    "    #optimizer.apply_gradients(zip(gradients_de, de.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_pe, pe.trainable_variables))\n",
    "    return loss_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs :  0\n",
      "loss depth 4.3044718e-05\n",
      "epochs :  1\n",
      "loss depth 1.1760587e-05\n",
      "epochs :  2\n",
      "loss depth 3.912331e-06\n",
      "epochs :  3\n",
      "loss depth 3.0100346e-06\n",
      "epochs :  4\n",
      "loss depth 5.2812197e-06\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "train_ds = []\n",
    "train_ps = []\n",
    "de = depth_estimation()\n",
    "pe = pose_est()\n",
    "\"\"\"\n",
    "for i in range(5):\n",
    "    #images_left = np.random.randint(0,255,(3,256,256,3)).astype(np.float32)\n",
    "    #images_right = np.random.randint(0,255,(3,256,256,3*2)).astype(np.float32)\n",
    "    images_left = tf.ones((3,128,416,3*2),dtype=tf.float32)*(i)\n",
    "    images_right = tf.ones((3,128,416,3*2),dtype=tf.float32)*(i+2)\n",
    "    train_ds.append([images_left,images_right])\n",
    "for j in range(5):\n",
    "    images_k = tf.ones((3,128,416,3*2))*(j)\n",
    "    images_k1 = tf.ones((3,128,416,3*2))*(j+1)\n",
    "    train_ps.append([images_k,images_k1])\n",
    "\"\"\"\n",
    "path_dir = \"/home/roboticist/Documents/Swaayatt/swaayatt_optical_flow/dataset_undeepVO/data_scene_flow(1)/training\"\n",
    "left_img_dir = path_dir+\"/image_2/\"\n",
    "right_img_dir = path_dir+\"/image_3/\"\n",
    "img_width = 416\n",
    "img_height = 128\n",
    "\n",
    "train_ds = data_loader_with_batch(left_img_dir,right_img_dir,img_width,img_height,batch=2,number_of_data=50)\n",
    "\n",
    "#de = depth_estimation()\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    for images_left,images_right in train_ds:\n",
    "        ld = train_step(images_left,images_right)\n",
    "\n",
    "    print(\"epochs : \",epoch)\n",
    "    print(\"loss depth\",ld.numpy())\n",
    "    #print(\"loss pose\",lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=3\n",
    "intrinsic = tf.expand_dims(tf.constant([[1,1,1],[1,1,1],[1,1,1]],dtype=tf.float32),axis=0)\n",
    "intrinsic = tf.tile(intrinsic,[batch,1,1])\n",
    "#intrinsic.dtype\n",
    "tf.linalg.inv(intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128, 416, 2)\n",
      "(2, 128, 416)\n",
      "tf.Tensor(\n",
      "[[[1.9969864 2.002676  2.0044484 ... 2.0424032 2.0537748 2.0043957]\n",
      "  [1.9953642 2.0000772 1.9995693 ... 1.9448109 1.9526278 1.9309225]\n",
      "  [1.9914441 1.9981756 1.9991022 ... 2.0341833 2.1246343 2.101088 ]\n",
      "  ...\n",
      "  [1.9922311 1.9892225 2.001728  ... 1.9941452 1.9999864 1.9929854]\n",
      "  [1.9975587 2.0160933 2.007829  ... 2.0019073 2.0081086 2.0005596]\n",
      "  [1.9950733 1.9955968 1.9928772 ... 1.9982989 1.9952387 1.9978765]]\n",
      "\n",
      " [[1.9904242 2.0091746 2.0173743 ... 2.009306  2.0230606 2.0284717]\n",
      "  [1.9934739 1.9966097 1.9996564 ... 1.9276782 1.9578804 1.9479724]\n",
      "  [1.9800311 1.9849877 1.9986681 ... 2.002211  2.0506854 2.0601249]\n",
      "  ...\n",
      "  [2.0008576 1.9942001 1.9962764 ... 2.0428753 2.0220377 1.9823285]\n",
      "  [1.9961455 2.0110211 2.0041008 ... 1.985572  2.0180545 1.9918451]\n",
      "  [1.9974968 1.9965783 1.9934357 ... 1.9888284 1.9853305 1.9847068]]], shape=(2, 128, 416), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_,dep_right = de(images_left)\n",
    "disp = dep_right[:,:,:,0]+epsilon\n",
    "depth = 1/disp\n",
    "print(dep_right.shape)\n",
    "print(disp.shape)\n",
    "print(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
