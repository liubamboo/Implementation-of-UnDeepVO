{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Conv2DTranspose,UpSampling2D,MaxPooling2D,Concatenate,BatchNormalization,Flatten\n",
    "from tensorflow.keras.layers import Dense,Layer\n",
    "import numpy as np\n",
    "from losses import *\n",
    "from nets.dep_network import depth_estimation\n",
    "from nets.pose_network import pose_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler2mat(z, y, x):\n",
    "    \"\"\"Converts euler angles to rotation matrix\n",
    "    TODO: remove the dimension for 'N' (deprecated for converting all source\n",
    "         poses altogether)\n",
    "    Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n",
    "    Args:\n",
    "      z: rotation angle along z axis (in radians) -- size = [B, N]\n",
    "      y: rotation angle along y axis (in radians) -- size = [B, N]\n",
    "      x: rotation angle along x axis (in radians) -- size = [B, N]\n",
    "    Returns:\n",
    "      Rotation matrix corresponding to the euler angles -- size = [B, N, 3, 3]\n",
    "    \"\"\"\n",
    "    B = tf.shape(z)[0]\n",
    "    N = 1\n",
    "    z = tf.clip_by_value(z, -np.pi, np.pi)\n",
    "    y = tf.clip_by_value(y, -np.pi, np.pi)\n",
    "    x = tf.clip_by_value(x, -np.pi, np.pi)\n",
    "\n",
    "    # Expand to B x N x 1 x 1\n",
    "    z = tf.expand_dims(tf.expand_dims(z, -1), -1)\n",
    "    y = tf.expand_dims(tf.expand_dims(y, -1), -1)\n",
    "    x = tf.expand_dims(tf.expand_dims(x, -1), -1)\n",
    "\n",
    "    zeros = tf.zeros([B, N, 1, 1])\n",
    "    ones  = tf.ones([B, N, 1, 1])\n",
    "\n",
    "    cosz = tf.cos(z)\n",
    "    sinz = tf.sin(z)\n",
    "    rotz_1 = tf.concat([cosz, -sinz, zeros], axis=3)\n",
    "    rotz_2 = tf.concat([sinz,  cosz, zeros], axis=3)\n",
    "    rotz_3 = tf.concat([zeros, zeros, ones], axis=3)\n",
    "    zmat = tf.concat([rotz_1, rotz_2, rotz_3], axis=2)\n",
    "\n",
    "    cosy = tf.cos(y)\n",
    "    siny = tf.sin(y)\n",
    "    roty_1 = tf.concat([cosy, zeros, siny], axis=3)\n",
    "    roty_2 = tf.concat([zeros, ones, zeros], axis=3)\n",
    "    roty_3 = tf.concat([-siny,zeros, cosy], axis=3)\n",
    "    ymat = tf.concat([roty_1, roty_2, roty_3], axis=2)\n",
    "\n",
    "    cosx = tf.cos(x)\n",
    "    sinx = tf.sin(x)\n",
    "    rotx_1 = tf.concat([ones, zeros, zeros], axis=3)\n",
    "    rotx_2 = tf.concat([zeros, cosx, -sinx], axis=3)\n",
    "    rotx_3 = tf.concat([zeros, sinx, cosx], axis=3)\n",
    "    xmat = tf.concat([rotx_1, rotx_2, rotx_3], axis=2)\n",
    "\n",
    "    rotMat = tf.matmul(tf.matmul(xmat, ymat), zmat)\n",
    "    return rotMat\n",
    "\n",
    "def pose_vec2mat(translation,rotation):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "      vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "    Returns:\n",
    "      A transformation matrix -- [B, 4, 4]\n",
    "    \"\"\"\n",
    "    batch_size, _ = translation.get_shape().as_list()\n",
    "    translation = tf.expand_dims(translation, -1)\n",
    "    rx = tf.slice(rotation, [0, 0], [-1, 1])\n",
    "    ry = tf.slice(rotation, [0, 1], [-1, 1])\n",
    "    rz = tf.slice(rotation, [0, 2], [-1, 1])\n",
    "    rot_mat = euler2mat(rz, ry, rx)\n",
    "    rot_mat = tf.squeeze(rot_mat, axis=[1])\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch_size, 1, 1])\n",
    "    transform_mat = tf.concat([rot_mat, translation], axis=2)\n",
    "    transform_mat = tf.concat([transform_mat, filler], axis=1)\n",
    "    return transform_mat\n",
    "\n",
    "def pixel2cam(depth, pixel_coords, intrinsics, is_homogeneous=True):\n",
    "    \"\"\"Transforms coordinates in the pixel frame to the camera frame.\n",
    "    Args:\n",
    "    depth: [batch, height, width]\n",
    "    pixel_coords: homogeneous pixel coordinates [batch, 3, height, width]\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    is_homogeneous: return in homogeneous coordinates\n",
    "    Returns:\n",
    "    Coords in the camera frame [batch, 3 (4 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    batch, height, width = depth.get_shape().as_list()\n",
    "    depth = tf.reshape(depth, [batch, 1, -1])\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 3, -1])\n",
    "    cam_coords = tf.matmul(tf.matrix_inverse(intrinsics), pixel_coords) * depth\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones([batch, 1, height*width])\n",
    "        cam_coords = tf.concat([cam_coords, ones], axis=1)\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, -1, height, width])\n",
    "    return cam_coords\n",
    "\n",
    "def cam2pixel(cam_coords, proj):\n",
    "    \"\"\"Transforms coordinates in a camera frame to the pixel frame.\n",
    "    Args:\n",
    "    cam_coords: [batch, 4, height, width]\n",
    "    proj: [batch, 4, 4]\n",
    "    Returns:\n",
    "    Pixel coordinates projected from the camera frame [batch, height, width, 2]\n",
    "    \"\"\"\n",
    "    batch, _, height, width = cam_coords.get_shape().as_list()\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, 4, -1])\n",
    "    unnormalized_pixel_coords = tf.matmul(proj, cam_coords)\n",
    "    x_u = tf.slice(unnormalized_pixel_coords, [0, 0, 0], [-1, 1, -1])\n",
    "    y_u = tf.slice(unnormalized_pixel_coords, [0, 1, 0], [-1, 1, -1])\n",
    "    z_u = tf.slice(unnormalized_pixel_coords, [0, 2, 0], [-1, 1, -1])\n",
    "    x_n = x_u / (z_u + 1e-10)\n",
    "    y_n = y_u / (z_u + 1e-10)\n",
    "    pixel_coords = tf.concat([x_n, y_n], axis=1)\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 2, height, width])\n",
    "    return tf.transpose(pixel_coords, perm=[0, 2, 3, 1])\n",
    "\n",
    "def meshgrid(batch, height, width, is_homogeneous=True):        \n",
    "    \"\"\"Construct a 2D meshgrid.\n",
    "    Args:\n",
    "    batch: batch size\n",
    "    height: height of the grid\n",
    "    width: width of the grid\n",
    "    is_homogeneous: whether to return in homogeneous coordinates\n",
    "    Returns:\n",
    "    x,y grid coordinates [batch, 2 (3 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n",
    "                  tf.transpose(tf.expand_dims(\n",
    "                      tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n",
    "    y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n",
    "                  tf.ones(shape=tf.stack([1, width])))\n",
    "    x_t = (x_t + 1.0) * 0.5 * tf.cast(width - 1, tf.float32)\n",
    "    y_t = (y_t + 1.0) * 0.5 * tf.cast(height - 1, tf.float32)\n",
    "    \"\"\"\n",
    "    width_f = tf.cast(width,tf.float32)\n",
    "    height_f = tf.cast(height,tf.float32)\n",
    "    x_t,y_t = tf.meshgrid(tf.linspace(0.0,width_f,width),tf.linspace(0.0,height_f,height))\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones_like(x_t)\n",
    "        coords = tf.stack([x_t, y_t, ones], axis=0)\n",
    "    else:\n",
    "        coords = tf.stack([x_t, y_t], axis=0)\n",
    "    coords = tf.tile(tf.expand_dims(coords, 0), [batch, 1, 1, 1])\n",
    "    return coords\n",
    "\n",
    "def projective_inverse_warp(img, depth, translate,rotate, intrinsics):\n",
    "    \"\"\"Inverse warp a source image to the target image plane based on projection.\n",
    "    Args:\n",
    "    img: the source image [batch, height_s, width_s, 3]\n",
    "    depth: depth map of the target image [batch, height_t, width_t]\n",
    "    pose: target to source camera transformation matrix [batch, 6], in the\n",
    "          order of tx, ty, tz, rx, ry, rz\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    Returns:\n",
    "    Source image inverse warped to the target image plane [batch, height_t,\n",
    "    width_t, 3]\n",
    "    \"\"\"\n",
    "    batch, height, width, _ = img.get_shape().as_list()\n",
    "    # Convert pose vector to matrix\n",
    "    pose = pose_vec2mat(translate,rotate)\n",
    "    # Construct pixel grid coordinates\n",
    "    pixel_coords = meshgrid(batch, height, width)\n",
    "    # Convert pixel coordinates to the camera frame\n",
    "    cam_coords = pixel2cam(depth, pixel_coords, intrinsics)\n",
    "    # Construct a 4x4 intrinsic matrix (TODO: can it be 3x4?)\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch, 1, 1])\n",
    "    intrinsics = tf.concat([intrinsics, tf.zeros([batch, 3, 1])], axis=2)\n",
    "    intrinsics = tf.concat([intrinsics, filler], axis=1)\n",
    "    # Get a 4x4 transformation matrix from 'target' camera frame to 'source'\n",
    "    # pixel frame.\n",
    "    proj_tgt_cam_to_src_pixel = tf.matmul(intrinsics, pose)\n",
    "    src_pixel_coords = cam2pixel(cam_coords, proj_tgt_cam_to_src_pixel)\n",
    "    output_img = bilinear_sampler_1d_h(img, src_pixel_coords)\n",
    "    return output_img \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe = pose_est()\n",
    "def pose_est_loss(rot_right,rot_left,t_left,t_right):\n",
    "    ##Spatial loss\n",
    "    #Pose consistency loss\n",
    "    lambd_p = 0.5\n",
    "    lambd_o = 0.5\n",
    "\n",
    "    pc_loss = lambd_p*tf.reduce_mean(tf.abs(t_right-t_left))+lambd_o*tf.reduce_mean(tf.abs(rot_right-rot_left))\n",
    "\n",
    "    #Temporal loss\n",
    "    \n",
    "    return pc_loss\n",
    "\n",
    "def pose_temporal_loss(pk,pk1,translate,rotate,intrinsics,depth):\n",
    "    \"\"\"\n",
    "    Argument :\n",
    "    pk is the pixel in the current frame \n",
    "    pk1 which is p_k+1 is the next frame pixel or next image \n",
    "    \"\"\"\n",
    "    \n",
    "    pk1 = projective_inverse_warp(pk, depth, translate,rotate, intrinsics)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "@tf.function\n",
    "def train_step(image_left,image_right):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        #dcx7_right,dep_right = de(image_right)\n",
    "        #dcx7_left,dep_left = de(image_left)\n",
    "        #loss_depth = dept_est_loss(dep_left,dep_right,image_left,image_right)\n",
    "        #tf.print(\"loss_depth\",loss_depth)\n",
    "        ##not using pose estimation currently because of tensorflow 2.0\n",
    "        t_left,rot_left = pe(image_left)\n",
    "        t_right,rot_right = pe(image_right)\n",
    "        loss_pose = pose_est_loss(rot_right,rot_left,t_left,t_right)\n",
    "        #tf.print(\"loss_pose\",loss_pose)\n",
    "\n",
    "    #gradients_de = tape.gradient(loss_depth, de.trainable_variables)\n",
    "    gradients_pe = tape.gradient(loss_pose, pe.trainable_variables)\n",
    "    #optimizer.apply_gradients(zip(gradients_de, de.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_pe, pe.trainable_variables))\n",
    "    return loss_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "train_ds = []\n",
    "\n",
    "#de = depth_estimation()\n",
    "pe = pose_est()\n",
    "\n",
    "for i in range(5):\n",
    "    #images_left = np.random.randint(0,255,(3,256,256,3)).astype(np.float32)\n",
    "    #images_right = np.random.randint(0,255,(3,256,256,3*2)).astype(np.float32)\n",
    "    images_left = tf.ones((3,256,256,3*2),dtype=tf.float32)*(i)\n",
    "    images_right = tf.ones((3,256,256,3*2),dtype=tf.float32)*(i+2)\n",
    "    train_ds.append([images_left,images_right])\n",
    "\n",
    "#images_left = tf.ones((1,256,256,3*2),dtype=tf.float32)*(0)\n",
    "#images_right = tf.ones((1,256,256,3*2),dtype=tf.float32)*(1)\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    for images_left,images_right in train_ds:\n",
    "        ld = train_step(images_left,images_right)\n",
    "    print(\"epochs : \",epoch)\n",
    "    print(\"loss depth\",ld.numpy())\n",
    "    #print(\"loss pose\",lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
