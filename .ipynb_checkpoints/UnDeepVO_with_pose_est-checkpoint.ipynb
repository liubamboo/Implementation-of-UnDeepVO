{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Conv2DTranspose,UpSampling2D,MaxPooling2D,Concatenate,BatchNormalization,Flatten\n",
    "from tensorflow.keras.layers import Dense,Layer\n",
    "import numpy as np\n",
    "from losses import *\n",
    "from nets.dep_network import depth_estimation\n",
    "from nets.pose_network import pose_est\n",
    "from utils import SSIM,bilinear_sampler_1d_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler2mat(z, y, x):\n",
    "    \"\"\"Converts euler angles to rotation matrix\n",
    "    TODO: remove the dimension for 'N' (deprecated for converting all source\n",
    "         poses altogether)\n",
    "    Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n",
    "    Args:\n",
    "      z: rotation angle along z axis (in radians) -- size = [B, N]\n",
    "      y: rotation angle along y axis (in radians) -- size = [B, N]\n",
    "      x: rotation angle along x axis (in radians) -- size = [B, N]\n",
    "    Returns:\n",
    "      Rotation matrix corresponding to the euler angles -- size = [B, N, 3, 3]\n",
    "    \"\"\"\n",
    "    B = tf.shape(z)[0]\n",
    "    N = 1\n",
    "    z = tf.clip_by_value(z, -np.pi, np.pi)\n",
    "    y = tf.clip_by_value(y, -np.pi, np.pi)\n",
    "    x = tf.clip_by_value(x, -np.pi, np.pi)\n",
    "\n",
    "    # Expand to B x N x 1 x 1\n",
    "    z = tf.expand_dims(tf.expand_dims(z, -1), -1)\n",
    "    y = tf.expand_dims(tf.expand_dims(y, -1), -1)\n",
    "    x = tf.expand_dims(tf.expand_dims(x, -1), -1)\n",
    "\n",
    "    zeros = tf.zeros([B, N, 1, 1])\n",
    "    ones  = tf.ones([B, N, 1, 1])\n",
    "\n",
    "    cosz = tf.cos(z)\n",
    "    sinz = tf.sin(z)\n",
    "    rotz_1 = tf.concat([cosz, -sinz, zeros], axis=3)\n",
    "    rotz_2 = tf.concat([sinz,  cosz, zeros], axis=3)\n",
    "    rotz_3 = tf.concat([zeros, zeros, ones], axis=3)\n",
    "    zmat = tf.concat([rotz_1, rotz_2, rotz_3], axis=2)\n",
    "\n",
    "    cosy = tf.cos(y)\n",
    "    siny = tf.sin(y)\n",
    "    roty_1 = tf.concat([cosy, zeros, siny], axis=3)\n",
    "    roty_2 = tf.concat([zeros, ones, zeros], axis=3)\n",
    "    roty_3 = tf.concat([-siny,zeros, cosy], axis=3)\n",
    "    ymat = tf.concat([roty_1, roty_2, roty_3], axis=2)\n",
    "\n",
    "    cosx = tf.cos(x)\n",
    "    sinx = tf.sin(x)\n",
    "    rotx_1 = tf.concat([ones, zeros, zeros], axis=3)\n",
    "    rotx_2 = tf.concat([zeros, cosx, -sinx], axis=3)\n",
    "    rotx_3 = tf.concat([zeros, sinx, cosx], axis=3)\n",
    "    xmat = tf.concat([rotx_1, rotx_2, rotx_3], axis=2)\n",
    "\n",
    "    rotMat = tf.matmul(tf.matmul(xmat, ymat), zmat)\n",
    "    return rotMat\n",
    "\n",
    "def pose_vec2mat(translation,rotation):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "      vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "    Returns:\n",
    "      A transformation matrix -- [B, 4, 4]\n",
    "    \"\"\"\n",
    "    batch_size, _ = translation.get_shape().as_list()\n",
    "    translation = tf.expand_dims(translation, -1)\n",
    "    rx = tf.slice(rotation, [0, 0], [-1, 1])\n",
    "    ry = tf.slice(rotation, [0, 1], [-1, 1])\n",
    "    rz = tf.slice(rotation, [0, 2], [-1, 1])\n",
    "    rot_mat = euler2mat(rz, ry, rx)\n",
    "    rot_mat = tf.squeeze(rot_mat, axis=[1])\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch_size, 1, 1])\n",
    "    transform_mat = tf.concat([rot_mat, translation], axis=2)\n",
    "    transform_mat = tf.concat([transform_mat, filler], axis=1)\n",
    "    return transform_mat\n",
    "\n",
    "def pose_inv_vec2mat(translation,rotation):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "      vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "    Returns:\n",
    "      A transformation matrix -- [B, 4, 4]\n",
    "    \"\"\"\n",
    "    batch_size, _ = translation.get_shape().as_list()\n",
    "    translation = tf.expand_dims(translation, -1)\n",
    "    rx = tf.slice(rotation, [0, 0], [-1, 1])\n",
    "    ry = tf.slice(rotation, [0, 1], [-1, 1])\n",
    "    rz = tf.slice(rotation, [0, 2], [-1, 1])\n",
    "    rot_mat = euler2mat(rz, ry, rx)\n",
    "    rot_mat = tf.squeeze(rot_mat, axis=[1])\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch_size, 1, 1])\n",
    "    transform_mat = tf.concat([rot_mat, translation], axis=2)\n",
    "    transform_mat = tf.concat([transform_mat, filler], axis=1)\n",
    "    return transform_mat\n",
    "\n",
    "def pixel2cam(depth, pixel_coords, intrinsics, is_homogeneous=True):\n",
    "    \"\"\"Transforms coordinates in the pixel frame to the camera frame.\n",
    "    Args:\n",
    "    depth: [batch, height, width]\n",
    "    pixel_coords: homogeneous pixel coordinates [batch, 3, height, width]\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    is_homogeneous: return in homogeneous coordinates\n",
    "    Returns:\n",
    "    Coords in the camera frame [batch, 3 (4 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    batch, height, width = depth.get_shape().as_list()\n",
    "    depth = tf.reshape(depth, [batch, 1, -1])\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 3, -1])\n",
    "    cam_coords = tf.matmul(tf.linalg.inv(intrinsics), pixel_coords) * depth\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones([batch, 1, height*width])\n",
    "        cam_coords = tf.concat([cam_coords, ones], axis=1)\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, -1, height, width])\n",
    "    return cam_coords\n",
    "\n",
    "def cam2pixel(cam_coords, proj):\n",
    "    \"\"\"Transforms coordinates in a camera frame to the pixel frame.\n",
    "    Args:\n",
    "    cam_coords: [batch, 4, height, width]\n",
    "    proj: [batch, 4, 4]\n",
    "    Returns:\n",
    "    Pixel coordinates projected from the camera frame [batch, height, width, 2]\n",
    "    \"\"\"\n",
    "    batch, _, height, width = cam_coords.get_shape().as_list()\n",
    "    cam_coords = tf.reshape(cam_coords, [batch, 4, -1])\n",
    "    unnormalized_pixel_coords = tf.matmul(proj, cam_coords)\n",
    "    x_u = tf.slice(unnormalized_pixel_coords, [0, 0, 0], [-1, 1, -1])\n",
    "    y_u = tf.slice(unnormalized_pixel_coords, [0, 1, 0], [-1, 1, -1])\n",
    "    z_u = tf.slice(unnormalized_pixel_coords, [0, 2, 0], [-1, 1, -1])\n",
    "    x_n = x_u / (z_u + 1e-10)\n",
    "    y_n = y_u / (z_u + 1e-10)\n",
    "    pixel_coords = tf.concat([x_n, y_n], axis=1)\n",
    "    pixel_coords = tf.reshape(pixel_coords, [batch, 2, height, width])\n",
    "    return tf.transpose(pixel_coords, perm=[0, 2, 3, 1])\n",
    "\n",
    "def meshgrid(batch, height, width, is_homogeneous=True):        \n",
    "    \"\"\"Construct a 2D meshgrid.\n",
    "    Args:\n",
    "    batch: batch size\n",
    "    height: height of the grid\n",
    "    width: width of the grid\n",
    "    is_homogeneous: whether to return in homogeneous coordinates\n",
    "    Returns:\n",
    "    x,y grid coordinates [batch, 2 (3 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n",
    "                  tf.transpose(tf.expand_dims(\n",
    "                      tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n",
    "    y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n",
    "                  tf.ones(shape=tf.stack([1, width])))\n",
    "    x_t = (x_t + 1.0) * 0.5 * tf.cast(width - 1, tf.float32)\n",
    "    y_t = (y_t + 1.0) * 0.5 * tf.cast(height - 1, tf.float32)\n",
    "    \"\"\"\n",
    "    width_f = tf.cast(width,tf.float32)\n",
    "    height_f = tf.cast(height,tf.float32)\n",
    "    x_t,y_t = tf.meshgrid(tf.linspace(0.0,width_f,width),tf.linspace(0.0,height_f,height))\n",
    "    if is_homogeneous:\n",
    "        ones = tf.ones_like(x_t)\n",
    "        coords = tf.stack([x_t, y_t, ones], axis=0)\n",
    "    else:\n",
    "        coords = tf.stack([x_t, y_t], axis=0)\n",
    "    coords = tf.tile(tf.expand_dims(coords, 0), [batch, 1, 1, 1])\n",
    "    return coords\n",
    "\n",
    "def projective_inverse_warp(img, depth, translate,rotate, intrinsics):\n",
    "    \"\"\"Inverse warp a source image to the target image plane based on projection.\n",
    "    Args:\n",
    "    img: the source image [batch, height_s, width_s, 3]\n",
    "    depth: depth map of the target image [batch, height_t, width_t]\n",
    "    pose: target to source camera transformation matrix [batch, 6], in the\n",
    "          order of tx, ty, tz, rx, ry, rz\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    Returns:\n",
    "    Source image inverse warped to the target image plane [batch, height_t,\n",
    "    width_t, 3]\n",
    "    \"\"\"\n",
    "    batch, height, width, _ = img.get_shape().as_list()\n",
    "    # Convert pose vector to matrix\n",
    "    pose = pose_vec2mat(translate,rotate)\n",
    "    # Construct pixel grid coordinates\n",
    "    pixel_coords = meshgrid(batch, height, width)\n",
    "    # Convert pixel coordinates to the camera frame\n",
    "    cam_coords = pixel2cam(depth, pixel_coords, intrinsics)\n",
    "    # Construct a 4x4 intrinsic matrix (TODO: can it be 3x4?)\n",
    "    filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])\n",
    "    filler = tf.tile(filler, [batch, 1, 1])\n",
    "    intrinsics = tf.concat([intrinsics, tf.zeros([batch, 3, 1])], axis=2)\n",
    "    intrinsics = tf.concat([intrinsics, filler], axis=1)\n",
    "    # Get a 4x4 transformation matrix from 'target' camera frame to 'source'\n",
    "    # pixel frame.\n",
    "    proj_tgt_cam_to_src_pixel = tf.matmul(intrinsics, pose)\n",
    "    src_pixel_coords = cam2pixel(cam_coords, proj_tgt_cam_to_src_pixel)\n",
    "    output_img = bilinear_sampler(img, src_pixel_coords)\n",
    "    return output_img \n",
    "def bilinear_sampler(imgs, coords):\n",
    "    \"\"\"Construct a new image by bilinear sampling from the input image.\n",
    "    Points falling outside the source image boundary have value 0.\n",
    "    Args:\n",
    "    imgs: source image to be sampled from [batch, height_s, width_s, channels]\n",
    "    coords: coordinates of source pixels to sample from [batch, height_t,\n",
    "      width_t, 2]. height_t/width_t correspond to the dimensions of the output\n",
    "      image (don't need to be the same as height_s/width_s). The two channels\n",
    "      correspond to x and y coordinates respectively.\n",
    "    Returns:\n",
    "    A new sampled image [batch, height_t, width_t, channels]\n",
    "    \"\"\"\n",
    "    def _repeat(x, n_repeats):        \n",
    "        rep = tf.transpose(\n",
    "            tf.expand_dims(tf.ones(shape=tf.stack([\n",
    "                n_repeats,\n",
    "            ])), 1), [1, 0])\n",
    "        rep = tf.cast(rep, 'float32')\n",
    "        x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "        return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "    coords_x, coords_y = tf.split(coords, [1, 1], axis=3)\n",
    "    inp_size = imgs.get_shape()\n",
    "    coord_size = coords.get_shape()\n",
    "    out_size = coords.get_shape().as_list()\n",
    "    out_size[3] = imgs.get_shape().as_list()[3]\n",
    "\n",
    "    coords_x = tf.cast(coords_x, 'float32')\n",
    "    coords_y = tf.cast(coords_y, 'float32')\n",
    "\n",
    "    x0 = tf.floor(coords_x)\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.floor(coords_y)\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    y_max = tf.cast(tf.shape(imgs)[1] - 1, 'float32')\n",
    "    x_max = tf.cast(tf.shape(imgs)[2] - 1, 'float32')\n",
    "    zero = tf.zeros([1], dtype='float32')\n",
    "\n",
    "    x0_safe = tf.clip_by_value(x0, zero, x_max)\n",
    "    y0_safe = tf.clip_by_value(y0, zero, y_max)\n",
    "    x1_safe = tf.clip_by_value(x1, zero, x_max)\n",
    "    y1_safe = tf.clip_by_value(y1, zero, y_max)\n",
    "\n",
    "    ## bilinear interp weights, with points outside the grid having weight 0\n",
    "    # wt_x0 = (x1 - coords_x) * tf.cast(tf.equal(x0, x0_safe), 'float32')\n",
    "    # wt_x1 = (coords_x - x0) * tf.cast(tf.equal(x1, x1_safe), 'float32')\n",
    "    # wt_y0 = (y1 - coords_y) * tf.cast(tf.equal(y0, y0_safe), 'float32')\n",
    "    # wt_y1 = (coords_y - y0) * tf.cast(tf.equal(y1, y1_safe), 'float32')\n",
    "\n",
    "    wt_x0 = x1_safe - coords_x\n",
    "    wt_x1 = coords_x - x0_safe\n",
    "    wt_y0 = y1_safe - coords_y\n",
    "    wt_y1 = coords_y - y0_safe\n",
    "\n",
    "    ## indices in the flat image to sample from\n",
    "    dim2 = tf.cast(inp_size[2], 'float32')\n",
    "    dim1 = tf.cast(inp_size[2] * inp_size[1], 'float32')\n",
    "    base = tf.reshape(\n",
    "        _repeat(\n",
    "            tf.cast(tf.range(coord_size[0]), 'float32') * dim1,\n",
    "            coord_size[1] * coord_size[2]),\n",
    "        [out_size[0], out_size[1], out_size[2], 1])\n",
    "\n",
    "    base_y0 = base + y0_safe * dim2\n",
    "    base_y1 = base + y1_safe * dim2\n",
    "    idx00 = tf.reshape(x0_safe + base_y0, [-1])\n",
    "    idx01 = x0_safe + base_y1\n",
    "    idx10 = x1_safe + base_y0\n",
    "    idx11 = x1_safe + base_y1\n",
    "\n",
    "    ## sample from imgs\n",
    "    imgs_flat = tf.reshape(imgs, tf.stack([-1, inp_size[3]]))\n",
    "    imgs_flat = tf.cast(imgs_flat, 'float32')\n",
    "    im00 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx00, 'int32')), out_size)\n",
    "    im01 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx01, 'int32')), out_size)\n",
    "    im10 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx10, 'int32')), out_size)\n",
    "    im11 = tf.reshape(tf.gather(imgs_flat, tf.cast(idx11, 'int32')), out_size)\n",
    "\n",
    "    w00 = wt_x0 * wt_y0\n",
    "    w01 = wt_x0 * wt_y1\n",
    "    w10 = wt_x1 * wt_y0\n",
    "    w11 = wt_x1 * wt_y1\n",
    "\n",
    "    output = tf.add_n([\n",
    "        w00 * im00, w01 * im01,\n",
    "        w10 * im10, w11 * im11\n",
    "    ])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pe = pose_est()\n",
    "def pose_est_loss(rot_right,rot_left,t_left,t_right):\n",
    "    ##Spatial loss\n",
    "    #Pose consistency loss\n",
    "    lambd_p = 0.5\n",
    "    lambd_o = 0.5\n",
    "\n",
    "    pc_loss = lambd_p*tf.reduce_mean(tf.abs(t_right-t_left))+lambd_o*tf.reduce_mean(tf.abs(rot_right-rot_left))\n",
    "\n",
    "    #Temporal loss\n",
    "    \n",
    "    return pc_loss\n",
    "\n",
    "def pose_temporal_loss(pk,pk1,translate,rotate,intrinsics,depth):\n",
    "    \"\"\"\n",
    "    Argument :\n",
    "    pk is the pixel in the current frame \n",
    "    pk1 which is p_k+1 is the next frame pixel or next image \n",
    "    \"\"\"\n",
    "    lambd = 0.5\n",
    "    pk1_pred = projective_inverse_warp(pk, depth, translate,rotate, intrinsics)\n",
    "    ssim_loss_pk1 = SSIM(pk1,pk1_pred) \n",
    "    l1_loss_pk1 = tf.reduce_mean(tf.abs(tf.subtract(pk1_pred,pk1)))\n",
    "    \n",
    "    loss_pk1 = lambd*ssim_loss_pk1+(1-lambd)*l1_loss_pk1\n",
    "    return loss_pk1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "        \n",
    "@tf.function\n",
    "def train_step(image_left,image_right,pk,pk1):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        intrinsic = tf.expand_dims(tf.constant([[1,1,1],[1,1,1],[1,1,1]],dtype=tf.float32),axis=0)\n",
    "        intrinsic = tf.tile(intrinsic,[batch,1,1])\n",
    "\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        dcx7_right,dep_right = de(image_right)\n",
    "        #dcx7_left,dep_left = de(image_left)\n",
    "        #loss_depth = dept_est_loss(dep_left,dep_right,image_left,image_right)\n",
    "        #tf.print(\"loss_depth\",loss_depth)\n",
    "        ##not using pose estimation currently because of tensorflow 2.0\n",
    "        t_left,rot_left = pe(image_left)\n",
    "        t_right,rot_right = pe(image_right)\n",
    "        loss_pose = pose_est_loss(rot_right,rot_left,t_left,t_right)\n",
    "        depth = 1/dep_right[:,:,:,0]\n",
    "        loss_pose_1 = pose_temporal_loss(pk,pk1,t_right,rot_right,intrinsic,depth)\n",
    "        #tf.print(\"loss_pose\",loss_pose)\n",
    "        loss_pose_total = loss_pose+loss_pose_1\n",
    "\n",
    "    #gradients_de = tape.gradient(loss_depth, de.trainable_variables)\n",
    "    gradients_pe = tape.gradient(loss_pose_total, pe.trainable_variables)\n",
    "    #optimizer.apply_gradients(zip(gradients_de, de.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_pe, pe.trainable_variables))\n",
    "    return loss_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " Input is not invertible.\n\t [[node MatrixInverse (defined at <ipython-input-26-7598b4782e9e>:104) ]] [Op:__inference_train_step_13070]\n\nFunction call stack:\ntrain_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f9830fe680ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Reset the metrics at the start of the next epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_k1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_k1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epochs : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/Documents/Swaayatt/swaayatt_optical_flow/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Input is not invertible.\n\t [[node MatrixInverse (defined at <ipython-input-26-7598b4782e9e>:104) ]] [Op:__inference_train_step_13070]\n\nFunction call stack:\ntrain_step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "train_ds = []\n",
    "train_ps = []\n",
    "de = depth_estimation()\n",
    "pe = pose_est()\n",
    "\n",
    "for i in range(5):\n",
    "    #images_left = np.random.randint(0,255,(3,256,256,3)).astype(np.float32)\n",
    "    #images_right = np.random.randint(0,255,(3,256,256,3*2)).astype(np.float32)\n",
    "    images_left = tf.ones((3,128,416,3*2),dtype=tf.float32)*(i)\n",
    "    images_right = tf.ones((3,128,416,3*2),dtype=tf.float32)*(i+2)\n",
    "    train_ds.append([images_left,images_right])\n",
    "for j in range(5):\n",
    "    images_k = tf.ones((3,128,416,3*2))*(j)\n",
    "    images_k1 = tf.ones((3,128,416,3*2))*(j+1)\n",
    "    train_ps.append([images_k,images_k1])\n",
    "#images_left = tf.ones((1,256,256,3*2),dtype=tf.float32)*(0)\n",
    "#images_right = tf.ones((1,256,256,3*2),dtype=tf.float32)*(1)\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    for (images_left,images_right),(images_k,images_k1) in zip(train_ds,train_ps):\n",
    "        ld = train_step(images_left,images_right,images_k,images_k1)\n",
    "    print(\"epochs : \",epoch)\n",
    "    print(\"loss depth\",ld.numpy())\n",
    "    #print(\"loss pose\",lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]], shape=(3, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
